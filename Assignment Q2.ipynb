{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('C:/Users/A30010587/Downloads/cwp_interview/cwp_interview/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import Optional, Dict, Any\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from utils import TimeseriesDataset, NormalizationIdentity, NormalizationStandardization, _split_series_time_dims, _merge_series_time_dims, _easy_mlp, device\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cvxpy as cp\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = pd.read_csv(\"Question2.csv\", index_col=0, header=[0,1], parse_dates=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = data[\"da\"]\n",
    "rt = data[\"rt\"]\n",
    "X = data[\"X\"]\n",
    "\n",
    "# example of prices with a two day lag if you wish to use timeseries as features (ie RNN, CNN, ARIMA, etc...)\n",
    "shifted_da = da.shift(freq=\"48H\")\n",
    "shifted_rt = rt.shift(freq=\"48H\")\n",
    "\n",
    "split = datetime(2020,8,1)\n",
    "\n",
    "X_train = X.loc[:split]\n",
    "X_validate = X.loc[split:]\n",
    "\n",
    "da_train = da.loc[:split]\n",
    "da_validate = da.loc[split:]\n",
    "\n",
    "rt_train = rt.loc[:split]\n",
    "rt_validate = rt.loc[split:]\n",
    "\n",
    "print('\\nVerify NaN values')\n",
    "print('X_train',X_train.isnull().values.any())\n",
    "print('Nan index in X_train',X_train.isnull().values.any(1).nonzero()[0])\n",
    "X_train.fillna(X_train.median(), inplace = True)\n",
    "print('X_train',X_train.isnull().values.any())\n",
    "print('X_validate',X_validate.isnull().values.any())\n",
    "print('da_train',da_train.isnull().values.any())\n",
    "print('da_validate',da_validate.isnull().values.any())\n",
    "print('rt_train',rt_train.isnull().values.any())\n",
    "print('rt_validate',rt_validate.isnull().values.any())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import TradingBot\n",
    "from QuantileDecoder import QuantileDecoder, AttentionalQuantile\n",
    "from utils import hourly_results, worst_loss, best_loss\n",
    "def RNN_model(gamma:float, l_norm:int):\n",
    "    model_name = 'RNN_model'\n",
    "    net = TradingBot(\n",
    "        num_series=100,\n",
    "        input_dim = 4,\n",
    "        gamma = gamma,\n",
    "        l_norm = l_norm,\n",
    "        data_normalization=\"standardization\",\n",
    "        loss_normalization=\"series\",\n",
    "        rnn_decoder={\n",
    "            \"dim_hidden_features\":2,\n",
    "            \"num_layers\":2,#32,\n",
    "            \"dim_output\":168 #predict one day ahead for the next week 24*7\n",
    "        },\n",
    "        \n",
    "    )\n",
    "    return net.to(device), model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L_vals = np.arange(0.1,0.9,0.1)\n",
    "L_norms = [1,2]\n",
    "pd_dict={}\n",
    "worst_loss_list = np.empty((len(L_norms), len(L_vals)))\n",
    "worst_return_list = []\n",
    "best_return_list = []\n",
    "for l_norm in L_norms:\n",
    "    for k, L_val in enumerate(L_vals):\n",
    "        nodes = 7\n",
    "        volume_short = np.zeros((len(L_vals), nodes))\n",
    "        volume_long = np.zeros((len(L_vals), nodes))\n",
    "        net, model_name = RNN_model(gamma= L_val, l_norm=l_norm)\n",
    "        optimizer = torch.optim.RMSprop(net.parameters(), lr=1e-3, weight_decay=0)\n",
    "\n",
    "        print(net)\n",
    "\n",
    "        avg_loss = []\n",
    "\n",
    "        NUM_EPOCHS = 50  # The model is very slow to train\n",
    "        NUM_BATCHES = 35\n",
    "\n",
    "        for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "            running_sum = 0\n",
    "            for batch in range(NUM_BATCHES):\n",
    "                running_sum += net.train_step(optimizer, 32, da_train.values.T, 24, 24)\n",
    "            avg_loss.append(running_sum / NUM_BATCHES)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(avg_loss)\n",
    "        plt.title('{}-Training Loss with L-{}-norm and gamma {}'.format(model_name,l_norm, round(L_val,4)))\n",
    "        plt.show()\n",
    "\n",
    "        v_long, bid, v_short, offer = net.predict(X_test, da, da_test, rt_test)\n",
    "        results = hourly_results(v_long, bid.values, v_short, offer.values, da_test, rt_test)\n",
    "        pnl = results.sum(axis=1).cumsum()\n",
    "        pnl.plot()\n",
    "        plt.title('{}-Cumulative Returns with L-{}-norm and gamma {}'.format(model_name,l_norm, round(L_val,4)))\n",
    "        plt.show()\n",
    "        portfolio_rets = bid.dot(v_long)+ offer.dot(v_short)\n",
    "        constraint = worst_loss(results)\n",
    "        best_hour = best_loss(results)\n",
    "        print('Portfolio returns',np.sum(portfolio_rets))\n",
    "        print('constraint',constraint)\n",
    "        worst_return_list.append(constraint)\n",
    "        best_return_list.append(best_hour)\n",
    "        for i in range(nodes):\n",
    "            volume_long[k, :] = v_long\n",
    "            volume_short[k, :] = v_short\n",
    "            pd_dict[l_norm, k] = pd.DataFrame({\"gamma\": f\"{round(L_vals[k],4)}\",\n",
    "                            \"v_shorts\":v_long,\n",
    "                            \"v_longs\":v_short})\n",
    "            pd_dict[l_norm,k]['L_norm'] = f\"L{l_norm}\"\n",
    "            \n",
    "        pd_dict[l_norm,k]['model'] = model_name\n",
    "        pd_dict[l_norm,k]['PTF_Return'] = np.sum(portfolio_rets)\n",
    "        pd_dict[l_norm,k]['Worst_Return'] = constraint\n",
    "        pd_dict[l_norm,k].loc[:, :'L_norm'].plot(kind=\"bar\")\n",
    "        plt.title(\"{}-L{} Norm with gamma {} and constraint {}\".format(model_name,l_norm,pd_dict[l_norm,k]['gamma'][0], round(pd_dict[l_norm,k]['Worst_Return'][0],4)) )\n",
    "        plt.xlabel(\"Nodes\")\n",
    "        plt.ylabel(\"Trades\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick the best startegy that satisfies our constraint and maximizes the returns\n",
    "\n",
    "The Model to consider based on vallidation dataset is : \n",
    "\n",
    "RNN model with gamma = 0.8 with L2-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_trades = pd_dict[(2,7)][['v_longs', 'v_shorts']]\n",
    "df_trades\n",
    "heatmap, ax = plt.subplots()\n",
    "plt.imshow(df_trades, cmap='plasma', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('RNN-Heatmap Trades of best startegy for gamma = 0.8 and L2 norm reg.')\n",
    "ax.set(xlabel='long     short', ylabel='Hub index')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the result of our startegy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(best_return_list, bins=20)\n",
    "plt.hist(worst_return_list, bins=10)\n",
    "plt.axvline(-1000, color='red', linestyle='solid')\n",
    "plt.axvline(constraint, color='red', linestyle='dashed')\n",
    "plt.legend(['Constraint = -1000',\n",
    "            'Worst return',\n",
    "            'Historical Returns Distribution', \n",
    "            'Returns < cVaR'])\n",
    "plt.title('RNN-Historical Return and CVaR')\n",
    "plt.xlabel('Return')\n",
    "plt.ylabel('Observation Frequency')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import TradingBot\n",
    "from utils import hourly_results, worst_loss\n",
    "def Transformers_model(gamma:float, l_norm:int):\n",
    "    model_name = 'Transformers'\n",
    "    net = TradingBot(\n",
    "        num_series=100,\n",
    "        input_dim = 12,\n",
    "        gamma = gamma,\n",
    "        l_norm = l_norm,\n",
    "        data_normalization=\"standardization\",\n",
    "        loss_normalization=\"series\",\n",
    "        series_embedding_dim=13,\n",
    "        input_encoder_layers=3,\n",
    "        input_encoding_normalization=True,\n",
    "        encoder= {\n",
    "            \"attention_layers\":3,\n",
    "            \"attention_heads\": 3,\n",
    "            \"attention_dim\": 4,\n",
    "            \"attention_feedforward_dim\": 12,\n",
    "        },\n",
    "        quantile_decoder={\n",
    "             \"min_u\": 0.01,\n",
    "             \"max_u\": 0.99,\n",
    "            \"attentional_quantile\": {\n",
    "                \"attention_heads\": 3,\n",
    "                \"attention_layers\": 3,\n",
    "                \"attention_dim\": 12,\n",
    "                \"mlp_layers\": 3,\n",
    "                \"mlp_dim\": 16,\n",
    "                \"resolution\": 50,\n",
    "            },\n",
    "        }\n",
    "    )\n",
    "    return net.to(device), model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "L_vals = np.arange(0.1,0.9,0.1)\n",
    "L_norms = [1,2]\n",
    "pd_dict={}\n",
    "worst_loss_list = np.empty((len(L_norms), len(L_vals)))\n",
    "worst_return_list = []\n",
    "best_return_list = []\n",
    "for l_norm in L_norms:\n",
    "    for k, L_val in enumerate(L_vals):\n",
    "        nodes = 7\n",
    "        volume_short = np.zeros((len(L_vals), nodes))\n",
    "        volume_long = np.zeros((len(L_vals), nodes))\n",
    "        net, model_name = Transformers_model(gamma= L_val, l_norm=l_norm)\n",
    "        optimizer = torch.optim.RMSprop(net.parameters(), lr=1e-3, weight_decay=0)\n",
    "\n",
    "        print(net)\n",
    "\n",
    "        avg_loss = []\n",
    "\n",
    "        NUM_EPOCHS = 50  # The model is very slow to train\n",
    "        NUM_BATCHES = 35\n",
    "\n",
    "        for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "            running_sum = 0\n",
    "            for batch in range(NUM_BATCHES):\n",
    "                running_sum += net.train_step(optimizer, 32, da_train.values.T, 24, 24)\n",
    "            avg_loss.append(running_sum / NUM_BATCHES)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(avg_loss)\n",
    "        plt.title('{}-Training Loss with L-{}-norm and gamma {}'.format(model_name,l_norm, round(L_val,4)))\n",
    "        plt.show()\n",
    "\n",
    "        v_long, bid, v_short, offer = net.predict(X_test, da, da_test, rt_test)\n",
    "        results = hourly_results(v_long, bid.values, v_short, offer.values, da_test, rt_test)\n",
    "        pnl = results.sum(axis=1).cumsum()\n",
    "        pnl.plot()\n",
    "        plt.title('{}-Cumulative Returns with L-{}-norm and gamma {}'.format(model_name,l_norm, round(L_val,4)))\n",
    "        plt.show()\n",
    "        portfolio_rets = bid.dot(v_long)+ offer.dot(v_short)\n",
    "        constraint = worst_loss(results)\n",
    "        best_hour = best_loss(results)\n",
    "        print('Portfolio returns',np.sum(portfolio_rets))\n",
    "        print('constraint',constraint)\n",
    "        worst_return_list.append(constraint)\n",
    "        best_return_list.append(best_hour)\n",
    "        for i in range(nodes):\n",
    "            volume_long[k, :] = v_long\n",
    "            volume_short[k, :] = v_short\n",
    "            pd_dict[l_norm, k] = pd.DataFrame({\"gamma\": f\"{round(L_vals[k],4)}\",\n",
    "                            \"v_shorts\":v_long,\n",
    "                            \"v_longs\":v_short})\n",
    "            pd_dict[l_norm,k]['L_norm'] = f\"L{l_norm}\"\n",
    "            \n",
    "        pd_dict[l_norm,k]['model'] = model_name\n",
    "        pd_dict[l_norm,k]['PTF_Return'] = np.sum(portfolio_rets)\n",
    "        pd_dict[l_norm,k]['Worst_Return'] = constraint\n",
    "        pd_dict[l_norm,k].loc[:, :'L_norm'].plot(kind=\"bar\")\n",
    "        plt.title(\"{}-L{} Norm with gamma {} and constraint {}\".format(model_name,l_norm,pd_dict[l_norm,k]['gamma'][0], round(pd_dict[l_norm,k]['Worst_Return'][0],4)) )\n",
    "        plt.xlabel(\"Nodes\")\n",
    "        plt.ylabel(\"Trades\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick the best startegy that satisfies our constraint and maximizes the returns\n",
    "\n",
    "The Model to consider based on vallidation dataset is : \n",
    "\n",
    "Transformer model with gamma = 0.8 with L2-norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_trades = pd_dict[(2,7)][['v_longs', 'v_shorts']]\n",
    "df_trades\n",
    "heatmap, ax = plt.subplots()\n",
    "plt.imshow(df_trades, cmap='plasma', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Transformers-Heatmap Trades of best startegy for gamma = 0.8 and L2 norm reg.')\n",
    "ax.set(xlabel='long     short', ylabel='Hub index')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize our startegy below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist(best_return_list, bins=20)\n",
    "plt.hist(worst_return_list, bins=10)\n",
    "plt.axvline(-1000, color='red', linestyle='solid')\n",
    "plt.axvline(constraint, color='red', linestyle='dashed')\n",
    "plt.legend(['Constraint = -1000',\n",
    "            'Worst return',\n",
    "            'Historical Returns Distribution', \n",
    "            'Returns < cVaR'])\n",
    "plt.title('Transformers-Historical Return and CVaR')\n",
    "plt.xlabel('Return')\n",
    "plt.ylabel('Observation Frequency')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CWP-assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
